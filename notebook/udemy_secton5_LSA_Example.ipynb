{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA : Latent Semantic Analysis\n",
    "\n",
    "### Major factor:  Synonymy and Polysemy,\n",
    "Latent variable or hidden variable, combine words with similar meanings, like Computer, PC, Laptop are highly correlated, we think about some latent variable or hidden variable that represent all of them hence the term Latent Semantic Analysis comes. The job of LSA is to find these hidden/latent variables and transform the original data into these new variables and hopefully the dimensionality of this new data is much smaller than the original . And this allows us to speed up our computations. LSA helps to solve the synonymy problem by combining correlated variables. But conflicting viewpoint is that whether or not it helps with Polysemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The underlying maths behind LSA:\n",
    "PCA(Principal Components Analysis) and SVD(Singular Value Decomposition to a term document matrix)\n",
    "LSA is basically the concept of PCA and SVD.\n",
    "\n",
    "PCA - PCA does a transformation on our input vectors z = Qx(This is the matrix multiplication). When you multiply a vector by a scalar then it gives us another vector in the same direction; but when you multiply a vector by a matrix, you could possibly get a vector in a different direction. PCA rotates our original input vectors; in another words its the same vecto but in a different cordinate system.\n",
    "\n",
    "\n",
    "PCA does three things for us:\n",
    "1) It decorrelates all of our input data, So our data in the new cordinate system has zero correlation between any two input feature\n",
    "2) It orders each new dimension in decreasing order by its information content, so the first dimension carries the most information, the second dimension carries the less information than first but more than the third and so on.\n",
    "3) The third thing it does is it allows us to reduce the dimensionality of our data. So if our original vocabulary was 1000 words, we might find that when we join all the words by how often they co-occur in each document, May be the total number of distinct latent terms is only 100, One perspective of PCA is that it does denoising, Noise is quite prevalent in NLP, because the vocabulary size is large. So by doing the smoothing or denoising we can actually generalize better when we look at new data.\n",
    "\n",
    "\n",
    "since cutting off any of the higher dimensions if they contain, say, less than 5% of the total information, wont result a big loss, Note that by removing information you are not always reducing predictive ability, \n",
    "\n",
    "The central item in all these idea is the covariance matrix. The diagonals of covariance matrix tell us the variance of that direction and the off-diagonals tells us how correlated two dimensions are with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example of LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'rstripe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4f5ae39b6375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwornet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstripe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/all_book_titles.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'rstripe'"
     ]
    }
   ],
   "source": [
    "wornet_lemmatizer = WordNetLemmatizer()\n",
    "titles = [lines.rstripe() for lines in open(\"data/all_book_titles.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
