{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Bag_of_Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning raw text into feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer() # it turns words into their base form, using this to make our vocabulary size not too large.\n",
    "positive_reviews = BeautifulSoup(open('positive.review').read(), \"lxml\")\n",
    "positive_reviews = positive_reviews.find_all(\"review_text\")\n",
    "negative_reviews = BeautifulSoup(open('negative.review').read(), \"lxml\")\n",
    "negative_reviews = negative_reviews.find_all(\"review_text\")\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)] # Just to balance the class, making the number positive review and negative review equal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = list(w for w in open('stopwords.txt'))\n",
    "with open('stopwords.txt') as fl:\n",
    "    stopwords = fl.read()\n",
    "\n",
    "stopwords = [word.strip().lower().replace('\"',\"\") for word in stopwords.split(\",\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just to check that everything is going appropriately or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"n't\" in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to return \"word tokens\" in review sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t)>2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aparajita/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# word_index_map will be the vocabulary size.\n",
    "word_index_map = {} # need to create the index for each word, so that each word will have each own index in final data word vector\n",
    "# this will also help us to get the size of our final data word vecctor and will map the words to indices\n",
    "current_index = 0 # counter that will get increase when it get a new word \n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking each token to create a data array in the form of bunch of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# positive_tokenized\n",
    "def tokens_to_vector(tokens,label): # putting the labels and the vectors in the same array just to make shuffle more easier.\n",
    "    x = np.zeros(len(word_index_map)+1) # vocabulary size is equal to data vector size and +1 is for labels\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t] # getting the index from the word index map\n",
    "        x[i] += 1 # setting x at that index\n",
    "    x = x/x.sum() # getting the average of occuring a word in an index.\n",
    "    x[-1]=label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.77\n"
     ]
    }
   ],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_index_map)+1)) \n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i = i+1\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "np.random.shuffle(data)\n",
    "x = data[:, :-1] # All rows and all the columns except the last column which is label\n",
    "y = data[:, -1] # All the rows with last column which is label\n",
    "xtrain = x[:-100,] # minus 100 rows and all the \n",
    "ytrain = y[:-100,]\n",
    "xtest = x[-100:,]\n",
    "ytest = y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain,ytrain)\n",
    "print \"Classification rate:\", model.score(xtest,ytest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(data[1999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(data)\n",
    "# print len(word_index_map)\n",
    "# word_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.5538737696797057\n",
      "best 0.9792344875129372\n",
      "worked -0.7581884795813263\n",
      "easy 1.4439601733015197\n",
      "happy 0.5347551467198292\n",
      "time -0.6313886125055916\n",
      "love 0.848270359705569\n",
      "case 0.525655789080087\n",
      "returned -0.6619931124055823\n",
      "cable 0.618907865723112\n",
      "small 0.6048746763878722\n",
      "customer -0.6518549963426044\n",
      "try -0.5375538439006626\n",
      "great 3.5093123482606945\n",
      "perfect 0.8570875694686895\n",
      "waste -0.8817182223218818\n",
      "highly 0.8340928016688753\n",
      "return -0.972467073187257\n",
      "not -4.1161120327743825\n",
      "support -0.835128690132794\n",
      "price 2.2018010341416554\n",
      "using 0.5073433764523109\n",
      "lot 0.5155157910497682\n",
      "poor -0.7152157018103458\n",
      "month -0.6174339851965599\n",
      "tried -0.7370379826542071\n",
      "pretty 0.5274772661671989\n",
      "used 0.9593442876617211\n",
      "quality 1.2537249244650759\n",
      "need 0.632337777756183\n",
      "problem 0.5272195753756331\n",
      "speaker 0.7587159827493186\n",
      "well 1.0011250098231845\n",
      "even -0.7552539965289927\n",
      "recommend 0.6699039442494071\n",
      "bad -0.6490519982322411\n",
      "item -0.7183917249026279\n",
      "thing -0.8460307127564552\n",
      "first -0.6296493227880874\n",
      "little 0.6482300083313574\n",
      "get -1.2041026311733833\n",
      "sound 1.0294509076989407\n",
      "money -0.8223025357294229\n",
      "two -0.6810559555627161\n",
      "back -1.3859228543558657\n",
      "use 1.5797192752925884\n",
      "comfortable 0.5325079446848758\n",
      "like 0.5823038346924666\n",
      "buy -0.8362948701191907\n",
      "excellent 1.1889644425263988\n",
      "memory 0.8394860444157874\n",
      "refund -0.5207927383481084\n",
      "good 1.7725745309565166\n",
      "fast 0.7977070238593009\n",
      "something -0.5224614820675797\n",
      "warranty -0.5106375084442515\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight>threshold or weight< -threshold:\n",
    "        print word, weight\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to improve the sentiment analysis?\n",
    "#### By using recursive neural network, Accuracy is relative not absolute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
